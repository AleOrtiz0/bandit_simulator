<!DOCTYPE html>
<html>

<head>
    <title>README.md</title>
    <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
    
<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

html,footer,header{
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Custom MD PDF CSS
 */
html,footer,header{
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";

 }
body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>
<link rel="stylesheet" href="file:///home/uumami/itam/bandit_simulator/R%3A%5C2.Travail%5C1.Enseignement%5CCours%5C_1.Outils%5C2.Developpement%5C1.SCSS%5Cmain.css" type="text/css"><link rel="stylesheet" href="file:///home/uumami/itam/bandit_simulator/D%3A%5Crdaros%5CCours%5C_1.Outils%5C2.Developpement%5C1.SCSS%5Cmain.css" type="text/css">
</head>

<body>
    <p>clave unica ______________________________</p>
<h1 id="problema-de-multi-bandas-multi-armed-bandit-teor%C3%ADa-e-implementaci%C3%B3n">Problema de Multi-Bandas (Multi-Armed Bandit): Teoría e Implementación</h1>
<p>La tarea se entrega por discord antes del miercoles de la siguiente clase. Incluye llenar cuidadosamente en latex todos los snippets mencionados aqui, mas el codigo ya sea con link a colab o al repositorio. No olviden poner su clave unica. La idea es que investiguen, entiendan y proponga una solucion al problema. Utilicen chatgpt y los tutoriales de la tarea (cursor especialmente) para hacer codigo y entender el problema.</p>
<p><strong>Nota</strong><br>
No pueden utilizar machine learning salvo regresion lineal si asi lo desean (no arboles, deep learning, etc..).</p>
<p>La proxima clase vamos a continuar con un ejercicio parecido, pero usando cadenas de markov. Vamos a modificar el bandit para que sea mas interesante ante cadenas de markov.</p>
<p><strong>Examen</strong><br>
El lunes hay examen sobre estos ejercicios a papel y lapiz, la calificacion sera el $min{examen, ejercicios}$, si $|examen - ejercicios|&lt;1$ entonces sera el $maximo$.</p>
<h2 id="1-introducci%C3%B3n-a-los-problemas-de-multi-bandas">1. Introducción a los Problemas de Multi-Bandas</h2>
<h3 id="11-definici%C3%B3n-y-enunciado-del-problema">1.1 Definición y Enunciado del Problema</h3>
<p>El problema de Multi-Bandas (MAB, por sus siglas en inglés) es un problema clásico en teoría de la decisión y aprendizaje por refuerzo. Su nombre surge del escenario de un jugador que enfrenta múltiples máquinas tragamonedas (a veces llamadas &quot;bandidos de un solo brazo&quot;), cada una con diferentes probabilidades de recompensa desconocidas. El jugador debe decidir qué máquinas jugar, en qué orden y cuántas veces, para maximizar su recompensa total.</p>
<p>En este modelo:</p>
<ul>
<li>Existen $K$ brazos (o acciones) diferentes.</li>
<li>Cada brazo, cuando se jala, otorga una recompensa extraída de una distribución de probabilidad específica de ese brazo.</li>
<li>Las distribuciones de recompensa son inicialmente desconocidas para el tomador de decisiones.</li>
<li>El objetivo es maximizar la recompensa acumulada a lo largo de una serie de jugadas.</li>
</ul>
<p>El problema captura la disyuntiva fundamental entre <strong>exploración</strong> (probar diferentes brazos para reunir información sobre sus distribuciones de recompensa) y <strong>explotación</strong> (elegir el brazo que actualmente parece ser el mejor).</p>
<h3 id="12-dilema-de-exploraci%C3%B3n-vs-explotaci%C3%B3n">1.2 Dilema de Exploración vs. Explotación</h3>
<p>Este dilema está en el corazón del problema de multi-bandas:</p>
<ul>
<li><strong>Exploración</strong>: Seleccionar brazos para aprender más sobre sus distribuciones de recompensa, potencialmente sacrificando recompensas inmediatas.</li>
<li><strong>Explotación</strong>: Seleccionar el brazo que actualmente parece ofrecer la mayor recompensa esperada en función de la información reunida hasta el momento.</li>
</ul>
<p>Equilibrar estos dos aspectos es crucial. Demasiada exploración desperdicia recursos en brazos subóptimos. Demasiada explotación puede impedir descubrir un brazo mejor.</p>
<h3 id="13-formulaci%C3%B3n-matem%C3%A1tica-general">1.3 Formulación Matemática General</h3>
<p>Formalicemos el problema estándar de bandas estocásticas:</p>
<ul>
<li>Sea $K$ el número de brazos.</li>
<li>Para cada brazo $i \in {1, 2, \ldots, K}$, existe una distribución de probabilidad desconocida $\mathcal{D}_i$ con media $\mu_i$.</li>
<li>En cada paso de tiempo $t \in {1, 2, \ldots, T}$:
<ul>
<li>El agente selecciona un brazo $a_t \in {1, 2, \ldots, K}$.</li>
<li>El agente recibe una recompensa $r_t \sim \mathcal{D}_{a_t}$.</li>
</ul>
</li>
<li>El objetivo es maximizar la recompensa acumulada $\sum_{t=1}^{T} r_t$.</li>
</ul>
<p>Alternativamente, el problema puede enmarcarse en términos de minimizar <strong>el arrepentimiento</strong>. El arrepentimiento se define como la diferencia entre la recompensa obtenida al seleccionar siempre el brazo óptimo y la recompensa realmente obtenida por el agente:</p>
<p>$\text{Regret}(T) = T \cdot \max_{i} \mu_i - \mathbb{E}\left[\sum_{t=1}^{T} r_t\right]$</p>
<h2 id="2-escenarios-de-informaci%C3%B3n-en-nuestro-entorno-de-bandas">2. Escenarios de Información en Nuestro Entorno de Bandas</h2>
<p>En nuestro entorno de multi-bandas, exploramos tres escenarios de información distintos, cada uno proporcionando al agente diferentes niveles de conocimiento:</p>
<h3 id="21-escenario-de-informaci%C3%B3n-completa">2.1 Escenario de Información Completa</h3>
<p>En este escenario, el agente observa:</p>
<ul>
<li>El número de turno actual.</li>
<li>El número total de turnos T.</li>
<li>La probabilidad de recompensa para el brazo 1 (p1).</li>
<li>El historial completo de acciones y recompensas pasadas.</li>
</ul>
<p>Este es el escenario más informativo, ya que el agente conoce la probabilidad de uno de los brazos directamente y puede inferir la del otro con base en las recompensas observadas.</p>
<h3 id="22-escenario-de-informaci%C3%B3n-parcial">2.2 Escenario de Información Parcial</h3>
<p>En este escenario, el agente observa:</p>
<ul>
<li>El número de turno actual.</li>
<li>El número total de turnos T.</li>
<li>La probabilidad de recompensa para el brazo 1 (p1).</li>
<li>El historial de acciones y recompensas pasadas.</li>
</ul>
<p>El agente conoce la probabilidad de un brazo pero debe aprender la del otro a través de la experimentación.</p>
<h3 id="23-escenario-de-solo-recompensa">2.3 Escenario de Solo Recompensa</h3>
<p>En este escenario, el agente observa:</p>
<ul>
<li>El número de turno actual.</li>
<li>El historial de acciones y recompensas pasadas.</li>
</ul>
<p>Este es el escenario más desafiante porque:</p>
<ol>
<li>El agente no conoce la probabilidad de ninguno de los dos brazos.</li>
<li>El agente no conoce el número total de turnos T.</li>
</ol>
<p>El agente debe aprender las probabilidades de ambos brazos mediante la experimentación y no puede optimizar su estrategia en función de la duración conocida del juego.</p>
<h2 id="3-entornos-de-bandas-en-nuestro-playground">3. Entornos de Bandas en Nuestro Playground</h2>
<p>Nuestro entorno implementa cuatro tipos diferentes de entornos de multi-bandas, cada uno con características distintas que afectan cómo cambian las probabilidades de los brazos a lo largo del tiempo.</p>
<h3 id="31-entorno-de-banda-fija">3.1 Entorno de Banda Fija</h3>
<h4 id="descripci%C3%B3n">Descripción</h4>
<p>En el entorno de Banda Fija, cada brazo tiene una probabilidad constante de recompensa durante todo el juego. Estas probabilidades se asignan aleatoriamente al inicio de cada juego (uniforme entre 0.01 y 0.99) y permanecen sin cambios.</p>
<h4 id="formulaci%C3%B3n-matem%C3%A1tica">Formulación Matemática</h4>
<ul>
<li>Dos brazos: $a \in {0, 1}$</li>
<li>Probabilidades fijas: $p_1, p_2 \in [0.01, 0.99]$</li>
<li>En el turno $t$, al seleccionar el brazo $a$:
<ul>
<li>Se recibe recompensa $r_t = 1$ con probabilidad $p_{a+1}$</li>
<li>Se recibe recompensa $r_t = 0$ con probabilidad $1 - p_{a+1}$</li>
</ul>
</li>
</ul>
<h4 id="decisi%C3%B3n-t-fijo">Decisión (T Fijo)</h4>
<h3 id="ejercicio"><strong>EJERCICIO</strong></h3>
<p><strong>RESPUESTA</strong><br>
Definir el problema de decisión para la Banda Fija con horizonte de tiempo conocido T = 100. ¿Cuál es la función objetivo? ¿Cuáles son las restricciones? ¿Cuál es la política óptima?</p>
<pre class="hljs"><code><div>







</div></code></pre>
<h4 id="decisi%C3%B3n-t-aleatorio">Decisión (T Aleatorio)</h4>
<h3 id="ejercicio"><strong>EJERCICIO</strong></h3>
<p><strong>RESPUESTA</strong><br>
Definir el problema de decisión para la Banda Fija con horizonte de tiempo desconocido T ~ Uniform(1, 300). ¿Cómo afecta el horizonte de tiempo aleatorio la estrategia óptima?</p>
<pre class="hljs"><code><div>







</div></code></pre>
<h3 id="32-entorno-de-banda-peri%C3%B3dica">3.2 Entorno de Banda Periódica</h3>
<h4 id="descripci%C3%B3n">Descripción</h4>
<p>En el entorno de Banda Periódica, la probabilidad de recompensa de cada brazo cambia cada k turnos (por defecto, k=10). En cada punto de cambio, se asignan nuevas probabilidades aleatorias (uniforme entre 0.01 y 0.99) a ambos brazos.</p>
<h4 id="formulaci%C3%B3n-matem%C3%A1tica">Formulación Matemática</h4>
<ul>
<li>Dos brazos: $a \in {0, 1}$</li>
<li>En el turno $t$, las probabilidades son:
<ul>
<li>$p_1(t) = p_1^{\lfloor t/k \rfloor}$, donde $p_1^j \sim \text{Uniform}(0.01, 0.99)$</li>
<li>$p_2(t) = p_2^{\lfloor t/k \rfloor}$, donde $p_2^j \sim \text{Uniform}(0.01, 0.99)$</li>
</ul>
</li>
<li>El superíndice $j = \lfloor t/k \rfloor$ indica el número de &quot;período&quot;.</li>
<li>En cada punto de cambio (cuando $t$ es divisible por $k$), se asignan nuevos valores aleatorios.</li>
</ul>
<h4 id="decisi%C3%B3n-t-fijo">Decisión (T Fijo)</h4>
<h3 id="ejercicio"><strong>EJERCICIO</strong></h3>
<p><strong>RESPUESTA</strong><br>
Definir el problema de decisión para la Banda Periódica con horizonte de tiempo conocido T = 100 y período k = 10. ¿Cómo abordarías la búsqueda de una estrategia óptima? ¿Qué información adicional sería valiosa rastrear?</p>
<pre class="hljs"><code><div>







</div></code></pre>
<h4 id="decisi%C3%B3n-t-aleatorio">Decisión (T Aleatorio)</h4>
<h3 id="ejercicio"><strong>EJERCICIO</strong></h3>
<p><strong>RESPUESTA</strong><br>
Definir el problema de decisión para la Banda Periódica con horizonte de tiempo desconocido T ~ Uniform(1, 300) y período k = 10. ¿Cómo interactúa la aleatoriedad en T con la naturaleza periódica del entorno?</p>
<pre class="hljs"><code><div>







</div></code></pre>
<h3 id="33-entorno-de-banda-din%C3%A1mica">3.3 Entorno de Banda Dinámica</h3>
<h4 id="descripci%C3%B3n">Descripción</h4>
<p>En el entorno de Banda Dinámica, las probabilidades de recompensa para ambos brazos cambian en cada turno. Cada turno se asignan probabilidades aleatorias completamente nuevas (uniforme entre 0.01 y 0.99) a ambos brazos.</p>
<h4 id="formulaci%C3%B3n-matem%C3%A1tica">Formulación Matemática</h4>
<ul>
<li>Dos brazos: $a \in {0, 1}$</li>
<li>En el turno $t$, las probabilidades son:
<ul>
<li>$p_1(t) \sim \text{Uniform}(0.01, 0.99)$</li>
<li>$p_2(t) \sim \text{Uniform}(0.01, 0.99)$</li>
</ul>
</li>
<li>Se generan nuevos valores aleatorios en cada turno.</li>
</ul>
<h4 id="decisi%C3%B3n-t-fijo">Decisión (T Fijo)</h4>
<h3 id="ejercicio"><strong>EJERCICIO</strong></h3>
<p><strong>RESPUESTA</strong><br>
Definir el problema de decisión para la Banda Dinámica con horizonte de tiempo conocido T = 100. ¿Hay una forma significativa de aprender de observaciones pasadas en este entorno? ¿Cuál sería la estrategia óptima?</p>
<pre class="hljs"><code><div>







</div></code></pre>
<h4 id="decisi%C3%B3n-t-aleatorio">Decisión (T Aleatorio)</h4>
<h3 id="ejercicio"><strong>EJERCICIO</strong></h3>
<p><strong>RESPUESTA</strong><br>
Definir el problema de decisión para la Banda Dinámica con horizonte de tiempo desconocido T ~ Uniform(1, 300). ¿Cambia significativamente el enfoque óptimo en este entorno altamente dinámico si el horizonte de tiempo es desconocido?</p>
<pre class="hljs"><code><div>







</div></code></pre>
<h3 id="34-entorno-de-banda-totalmente-aleatorio">3.4 Entorno de Banda Totalmente Aleatorio</h3>
<h4 id="descripci%C3%B3n">Descripción</h4>
<p>En el entorno de Banda Totalmente Aleatorio, las probabilidades de los brazos se inicializan de forma aleatoria y luego cambian aleatoriamente con una pequeña probabilidad (5%) en cada turno. Esto crea un entorno donde los cambios son impredecibles pero ocurren con menos frecuencia que en el entorno Dinámico.</p>
<h4 id="formulaci%C3%B3n-matem%C3%A1tica">Formulación Matemática</h4>
<ul>
<li>Dos brazos: $a \in {0, 1}$</li>
<li>Probabilidades iniciales: $p_1(0), p_2(0) \sim \text{Uniform}(0.01, 0.99)$</li>
<li>En el turno $t &gt; 0$, con probabilidad 0.05:
<ul>
<li>$p_1(t) \sim \text{Uniform}(0.01, 0.99)$</li>
<li>$p_2(t) \sim \text{Uniform}(0.01, 0.99)$</li>
</ul>
</li>
<li>De lo contrario (con probabilidad 0.95):
<ul>
<li>$p_1(t) = p_1(t-1)$</li>
<li>$p_2(t) = p_2(t-1)$</li>
</ul>
</li>
</ul>
<h4 id="decisi%C3%B3n-t-fijo">Decisión (T Fijo)</h4>
<h3 id="ejercicio"><strong>EJERCICIO</strong></h3>
<p><strong>RESPUESTA</strong><br>
Definir el problema de decisión para la Banda Totalmente Aleatoria con horizonte de tiempo conocido T = 100. ¿Cómo equilibrarías la exploración y explotación sabiendo que las probabilidades de los brazos podrían cambiar repentinamente?</p>
<pre class="hljs"><code><div>







</div></code></pre>
<h4 id="decisi%C3%B3n-t-aleatorio">Decisión (T Aleatorio)</h4>
<h3 id="ejercicio"><strong>EJERCICIO</strong></h3>
<p><strong>RESPUESTA</strong><br>
Definir el problema de decisión para la Banda Totalmente Aleatoria con horizonte de tiempo desconocido T ~ Uniform(1, 300). ¿Cómo interactúan las dos formas de aleatoriedad (en las probabilidades de los brazos y en el horizonte de tiempo)?</p>
<pre class="hljs"><code><div>







</div></code></pre>
<h2 id="4-implementaci%C3%B3n-de-agentes">4. Implementación de Agentes</h2>
<p>En nuestro entorno, implementarás tres tipos de agentes correspondientes a los tres escenarios de información descritos anteriormente. Esto es lo que cada agente debe manejar:</p>
<h3 id="41-agente-de-informaci%C3%B3n-completa">4.1 Agente de Información Completa</h3>
<p><strong>Entrada:</strong></p>
<pre class="hljs"><code><div>env_info = {
    <span class="hljs-string">'current_turn'</span>: int,        <span class="hljs-comment"># Número de turno actual</span>
    <span class="hljs-string">'total_turns'</span>: int,         <span class="hljs-comment"># Número total de turnos en el juego</span>
    <span class="hljs-string">'p1'</span>: float,                <span class="hljs-comment"># Probabilidad de recompensa del brazo 1</span>
    <span class="hljs-string">'history'</span>: {
        <span class="hljs-string">'actions'</span>: [int, ...],   <span class="hljs-comment"># Acciones pasadas (0 para brazo 1, 1 para brazo 2)</span>
        <span class="hljs-string">'rewards'</span>: [float, ...], <span class="hljs-comment"># Recompensas pasadas</span>
        <span class="hljs-string">'p1'</span>: [float, ...],      <span class="hljs-comment"># Historial de probabilidades del brazo 1</span>
        <span class="hljs-string">'p2'</span>: [float, ...]       <span class="hljs-comment"># Historial de probabilidades del brazo 2 (solo para evaluación)</span>
    }
}
</div></code></pre>
<p><strong>Salida:</strong></p>
<pre class="hljs"><code><div>action = <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> <span class="hljs-number">1</span>  <span class="hljs-comment"># 0 para el brazo 1, 1 para el brazo 2</span>
</div></code></pre>
<h3 id="42-agente-de-informaci%C3%B3n-parcial">4.2 Agente de Información Parcial</h3>
<p><strong>Entrada:</strong></p>
<pre class="hljs"><code><div>env_info = {
    <span class="hljs-string">'current_turn'</span>: int,        <span class="hljs-comment"># Número de turno actual</span>
    <span class="hljs-string">'total_turns'</span>: int,         <span class="hljs-comment"># Número total de turnos en el juego</span>
    <span class="hljs-string">'p1'</span>: float,                <span class="hljs-comment"># Probabilidad de recompensa del brazo 1</span>
    <span class="hljs-string">'history'</span>: {
        <span class="hljs-string">'actions'</span>: [int, ...],   <span class="hljs-comment"># Acciones pasadas (0 para brazo 1, 1 para brazo 2)</span>
        <span class="hljs-string">'rewards'</span>: [float, ...]  <span class="hljs-comment"># Recompensas pasadas</span>
    }
}
</div></code></pre>
<p><strong>Salida:</strong></p>
<pre class="hljs"><code><div>action = <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> <span class="hljs-number">1</span>  <span class="hljs-comment"># 0 para el brazo 1, 1 para el brazo 2</span>
</div></code></pre>
<h3 id="43-agente-de-solo-recompensa">4.3 Agente de Solo Recompensa</h3>
<p><strong>Entrada:</strong></p>
<pre class="hljs"><code><div>env_info = {
    <span class="hljs-string">'current_turn'</span>: int,        <span class="hljs-comment"># Número de turno actual</span>
    <span class="hljs-string">'history'</span>: {
        <span class="hljs-string">'actions'</span>: [int, ...],   <span class="hljs-comment"># Acciones pasadas (0 para brazo 1, 1 para brazo 2)</span>
        <span class="hljs-string">'rewards'</span>: [float, ...]  <span class="hljs-comment"># Recompensas pasadas</span>
    }
}
</div></code></pre>
<p><strong>Salida:</strong></p>
<pre class="hljs"><code><div>action = <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> <span class="hljs-number">1</span>  <span class="hljs-comment"># 0 para el brazo 1, 1 para el brazo 2</span>
</div></code></pre>
<h2 id="5-m%C3%A9tricas-de-rendimiento">5. Métricas de Rendimiento</h2>
<p>El entorno evalúa el rendimiento de los agentes usando varias métricas clave:</p>
<h3 id="51-recompensa-promedio">5.1 Recompensa Promedio</h3>
<p>Esta es la recompensa media obtenida por turno, calculada como:</p>
<p>$\text{Recompensa Promedio} = \frac{1}{T} \sum_{t=1}^{T} r_t$</p>
<p>Esta métrica mide directamente qué tan bien el agente está maximizando su función objetivo. Valores más altos indican un mejor rendimiento.</p>
<h3 id="52-porcentaje-de-acciones-%C3%B3ptimas">5.2 Porcentaje de Acciones Óptimas</h3>
<p>Esta métrica mide el porcentaje de veces que el agente seleccionó el brazo con la mayor probabilidad de recompensa:</p>
<p>$\text{Acciones Óptimas (%)} = \frac{100}{T} \sum_{t=1}^{T} \mathbf{1}{a_t = \arg\max_i p_i(t)}$</p>
<p>Donde $\mathbf{1}$ es la función indicadora que vale 1 cuando la condición es verdadera y 0 en caso contrario.</p>
<p>Esta métrica muestra con qué frecuencia el agente elige el mejor brazo, independientemente de la recompensa real recibida. Valores más altos indican una mejor selección de brazos.</p>
<h3 id="53-arrepentimiento-regret">5.3 Arrepentimiento (Regret)</h3>
<p>El arrepentimiento mide la diferencia entre la recompensa esperada de elegir siempre el brazo óptimo y la recompensa esperada de las elecciones del agente:</p>
<p>$\text{Regret} = \sum_{t=1}^{T} \max_i p_i(t) - \sum_{t=1}^{T} p_{a_t+1}(t)$</p>
<p>Valores más bajos de arrepentimiento indican un mejor rendimiento.</p>
<h3 id="54-distribuci%C3%B3n-de-recompensas">5.4 Distribución de Recompensas</h3>
<p>El entorno visualiza la distribución de recompensas en diferentes entornos usando diagramas de caja (boxplots) y diagramas de violín (violin plots). Estas visualizaciones ayudan a entender:</p>
<ul>
<li>La mediana del rendimiento</li>
<li>La variabilidad en el rendimiento</li>
<li>La presencia de valores atípicos</li>
<li>La forma general de la distribución de recompensas</li>
</ul>
<h2 id="6-pautas-de-estrategia">6. Pautas de Estrategia</h2>
<h3 id="61-enfoques-generales">6.1 Enfoques Generales</h3>
<p>Aquí hay algunos enfoques generales a considerar para la implementación de tus agentes:</p>
<ol>
<li><strong>Selección Aleatoria</strong>: Elegir brazos aleatoriamente (enfoque de referencia).</li>
<li><strong>Greedy (Codicioso)</strong>: Elegir siempre el brazo con la recompensa estimada más alta.</li>
<li><strong>ε-Greedy</strong>: Casi siempre elegir el mejor brazo, pero explorar ocasionalmente.</li>
<li><strong>UCB (Upper Confidence Bound)</strong>: Elegir brazos basados en estimaciones optimistas de su valor.</li>
<li><strong>Thompson Sampling</strong>: Elegir brazos basados en emparejar probabilidades con distribuciones a posteriori.</li>
<li><strong>Enfoques Bayesianos</strong>: Mantener distribuciones de probabilidad sobre los valores de los brazos.</li>
</ol>
<h3 id="62-consideraciones-espec%C3%ADficas-del-entorno">6.2 Consideraciones Específicas del Entorno</h3>
<h4 id="banda-fija">Banda Fija</h4>
<ul>
<li>Enfocarse en identificar rápidamente el mejor brazo.</li>
<li>La exploración se vuelve menos valiosa conforme avanza el juego.</li>
<li>Con T conocido, se puede planificar un programa decreciente de exploración.</li>
</ul>
<h4 id="banda-peri%C3%B3dica">Banda Periódica</h4>
<ul>
<li>Detectar la estructura periódica (k=10).</li>
<li>Restablecer estimaciones al comienzo de cada período.</li>
<li>Asignar más exploración al inicio de cada período.</li>
</ul>
<h4 id="banda-din%C3%A1mica">Banda Dinámica</h4>
<ul>
<li>Las observaciones recientes valen más que las antiguas.</li>
<li>Considerar el uso de una ventana deslizante de observaciones.</li>
<li>Podría necesitar alta capacidad de respuesta a los cambios.</li>
</ul>
<h4 id="banda-totalmente-aleatoria">Banda Totalmente Aleatoria</h4>
<ul>
<li>Estar alerta a cambios repentinos en los patrones de recompensa.</li>
<li>Equilibrar la persistencia (usar historial) con la adaptabilidad.</li>
<li>Considerar métodos de detección de cambios.</li>
</ul>
<h3 id="63-consideraciones-espec%C3%ADficas-de-la-informaci%C3%B3n">6.3 Consideraciones Específicas de la Información</h3>
<h4 id="agente-de-informaci%C3%B3n-completa">Agente de Información Completa</h4>
<ul>
<li>Aprovechar el valor conocido p1.</li>
<li>Enfocarse en estimar p2 con eficiencia.</li>
<li>Ajustar la estrategia dinámicamente con base en los valores relativos.</li>
</ul>
<h4 id="agente-de-informaci%C3%B3n-parcial">Agente de Información Parcial</h4>
<ul>
<li>Similar a información completa, pero más limitado.</li>
<li>Podría requerir más exploración en ciertos entornos.</li>
</ul>
<h4 id="agente-de-solo-recompensa">Agente de Solo Recompensa</h4>
<ul>
<li>Debe estimar las probabilidades de ambos brazos.</li>
<li>Necesita lidiar con el horizonte de tiempo desconocido.</li>
<li>Considerar estrategias adaptativas en el tiempo.</li>
</ul>
<h2 id="7-conclusi%C3%B3n">7. Conclusión</h2>
<p>El problema de Multi-Bandas ofrece un marco fundamental para estudiar la toma de decisiones secuenciales bajo incertidumbre. Los entornos y escenarios de información en este playground brindan un conjunto rico de desafíos que resaltan diferentes aspectos del dilema exploración-explotación.</p>
<p>Al implementar agentes para estos escenarios, obtendrás experiencia práctica con conceptos clave en aprendizaje por refuerzo y teoría de la decisión, y desarrollarás intuición para equilibrar la recolección de información con la maximización de recompensas en diversos contextos.</p>
<p>Mientras trabajas en tus implementaciones, considera cómo se extenderían tus estrategias a:</p>
<ul>
<li>Bandas con más de dos brazos.</li>
<li>Espacios de acción continuos.</li>
<li>Distribuciones de recompensa no estacionarias con diferentes patrones.</li>
<li>Bandas contextuales donde se dispone de información adicional.</li>
</ul>

</body>

</html>